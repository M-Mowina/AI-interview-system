{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Interview System - Organized Version\n",
    "\n",
    "This notebook implements an AI-powered interview system using LangGraph, Google Gemini, and various speech technologies.\n",
    "\n",
    "## Features\n",
    "- ü§ñ AI Interviewer with customizable personality\n",
    "- üìö Knowledge base integration for position-specific questions\n",
    "- üìÑ Resume analysis and project-based questioning\n",
    "- üé§ Speech-to-Text (STT) for voice input\n",
    "- üîä Text-to-Speech (TTS) for voice output\n",
    "- üîÑ Real-time conversation flow with LangGraph\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Dependencies](#setup)\n",
    "2. [Core Components](#core-components)\n",
    "3. [Knowledge Base Setup](#knowledge-base)\n",
    "4. [Interview Agent](#interview-agent)\n",
    "5. [Speech Technologies](#speech)\n",
    "6. [Integration and Testing](#integration)\n",
    "7. [Usage Examples](#usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain_google_genai dotenv langgraph langchain_community langchain_chroma pypdf\n",
    "!pip install elevenlabs assemblyai sounddevice soundfile pyaudio websocket-client scipy speech_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Dict, TypedDict, List, Union, Annotated, Sequence\n",
    "from IPython.display import display, Markdown, Image, Audio\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, AnyMessage, BaseMessage, ToolMessage\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_chroma import Chroma\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and embeddings\n",
    "def initialize_models():\n",
    "    \"\"\"Initialize the language model and embeddings.\"\"\"\n",
    "    try:\n",
    "        llm = init_chat_model(\"google_genai:gemini-2.5-flash-lite-preview-06-17\")\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "        \n",
    "        # Test the models\n",
    "        test_response = llm.invoke(input=[HumanMessage(content=\"Hello\")])\n",
    "        test_embedding = embeddings.embed_query(\"Hello\")\n",
    "        \n",
    "        print(f\"‚úÖ LLM initialized successfully\")\n",
    "        print(f\"‚úÖ Embeddings initialized successfully (dimension: {len(test_embedding)})\")\n",
    "        \n",
    "        return llm, embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing models: {e}\")\n",
    "        raise\n",
    "\n",
    "llm, embeddings = initialize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Components <a name=\"core-components\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent state structure\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    State management for the recruiter agent.\n",
    "    \n",
    "    Attributes:\n",
    "        mode: Interview style (friendly, formal, technical)\n",
    "        num_of_q: Number of main questions to ask\n",
    "        num_of_follow_up: Number of follow-up questions allowed\n",
    "        position: Job position being interviewed for\n",
    "        company_name: Company conducting the interview\n",
    "        messages: Conversation history\n",
    "    \"\"\"\n",
    "    mode: str\n",
    "    num_of_q: int\n",
    "    num_of_follow_up: int\n",
    "    position: str\n",
    "    company_name: str\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Base Setup <a name=\"knowledge-base\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_knowledge_base():\n",
    "    \"\"\"\n",
    "    Set up the knowledge base with interview questions and resume data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (questions_retriever, resume_retriever)\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    pdf_path = \"../utils/LLM Interview Questions.pdf\"\n",
    "    resume_path = \"../utils/Mohamed-Mowina-AI-Resume.pdf\"\n",
    "    \n",
    "    # Validate file existence\n",
    "    for path in [pdf_path, resume_path]:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "    \n",
    "    try:\n",
    "        # Load documents\n",
    "        pdf_loader = PyPDFLoader(pdf_path)\n",
    "        resume_loader = PyPDFLoader(resume_path)\n",
    "        \n",
    "        pages = pdf_loader.load()\n",
    "        resume = resume_loader.load()\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(pages)} pages from interview questions\")\n",
    "        print(f\"‚úÖ Loaded {len(resume)} pages from resume\")\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        \n",
    "        pages_split = text_splitter.split_documents(pages)\n",
    "        resume_split = text_splitter.split_documents(resume)\n",
    "        \n",
    "        print(f\"‚úÖ Split into {len(pages_split)} question chunks\")\n",
    "        print(f\"‚úÖ Split into {len(resume_split)} resume chunks\")\n",
    "        \n",
    "        # Create vector stores\n",
    "        questions_vectorstore = Chroma.from_documents(\n",
    "            documents=pages_split, \n",
    "            embedding=embeddings, \n",
    "            collection_name=\"interview_questions\", \n",
    "            persist_directory=\"./data/interview_questions\"\n",
    "        )\n",
    "        \n",
    "        resume_vectorstore = Chroma.from_documents(\n",
    "            documents=resume_split, \n",
    "            embedding=embeddings, \n",
    "            collection_name=\"resume\", \n",
    "            persist_directory=\"./data/resume\"\n",
    "        )\n",
    "        \n",
    "        # Create retrievers\n",
    "        questions_retriever = questions_vectorstore.as_retriever()\n",
    "        resume_retriever = resume_vectorstore.as_retriever()\n",
    "        \n",
    "        print(\"‚úÖ Knowledge base setup complete\")\n",
    "        \n",
    "        return questions_retriever, resume_retriever\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up knowledge base: {e}\")\n",
    "        raise\n",
    "\n",
    "# Setup knowledge base\n",
    "questions_retriever, resume_retriever = setup_knowledge_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever tools\n",
    "def create_tools():\n",
    "    \"\"\"Create the retriever tools for the agent.\"\"\"\n",
    "    questions_tool = create_retriever_tool(\n",
    "        questions_retriever,\n",
    "        \"retrieve_questions\",\n",
    "        \"Search and return questions related to the position from the knowledge base.\"\n",
    "    )\n",
    "    \n",
    "    resume_tool = create_retriever_tool(\n",
    "        resume_retriever,\n",
    "        \"retrieve_resume\",\n",
    "        \"Search resume and return related projects and experience relevant to the position.\"\n",
    "    )\n",
    "    \n",
    "    return [questions_tool, resume_tool]\n",
    "\n",
    "tools = create_tools()\n",
    "print(f\"‚úÖ Created {len(tools)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interview Agent <a name=\"interview-agent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interviewer prompt template\n",
    "interviewer_prompt = PromptTemplate(\n",
    "    input_variables=[\"mode\", \"company_name\", \"position\", \"number_of_questions\", \"number_of_followup\"],\n",
    "    template=\"\"\"\n",
    "You are a {mode} AI interviewer for {company_name}, conducting an interview for a {position} position.\n",
    "\n",
    "Your goal is to assess the candidate's technical skills, problem-solving abilities, communication skills, and experience relevant to the position.\n",
    "\n",
    "You have access to two tools:\n",
    "1. `retrieve_questions`: Search for position-specific interview questions\n",
    "2. `retrieve_resume`: Search the candidate's resume for relevant projects and experience\n",
    "\n",
    "Interview Structure:\n",
    "- Start with a friendly introduction\n",
    "  - Ask the candidate to introduce themselves\n",
    "  - Ask about a specific project from their resume\n",
    "- Ask {number_of_questions} main questions from the knowledge base\n",
    "- Ask up to {number_of_followup} follow-up questions if answers are vague\n",
    "- End with \"Thank you, that's it for today.\"\n",
    "\n",
    "Guidelines:\n",
    "- Maintain a {mode} tone throughout\n",
    "- Number your questions clearly (Question 1, Question 2, etc.)\n",
    "- If asked irrelevant questions, respond with \"Sorry, this is out of scope.\"\n",
    "- Print \"tool used: `tool_name`\" when using tools\n",
    "\n",
    "Begin the interview now.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recruiter_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    The main recruiter agent function that processes the conversation.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        \n",
    "    Returns:\n",
    "        Updated agent state\n",
    "    \"\"\"\n",
    "    # Create system prompt\n",
    "    sys_prompt = SystemMessage(content=interviewer_prompt.format(\n",
    "        mode=state['mode'],\n",
    "        company_name=state['company_name'],\n",
    "        position=state['position'],\n",
    "        number_of_questions=state['num_of_q'],\n",
    "        number_of_followup=state['num_of_follow_up']\n",
    "    ))\n",
    "    \n",
    "    # Prepare messages for LLM\n",
    "    all_messages = [sys_prompt] + state[\"messages\"]\n",
    "    \n",
    "    # Invoke LLM with tools\n",
    "    response = llm.bind_tools(tools).invoke(all_messages)\n",
    "    \n",
    "    return {\"messages\": response}\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determine the next step in the conversation flow.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state\n",
    "        \n",
    "    Returns:\n",
    "        Next action: 'invoke_tools', 'continue_convo', or 'end'\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check if agent wants to use tools\n",
    "    if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "        return \"invoke_tools\"\n",
    "    \n",
    "    # Check if interview is finished\n",
    "    if \"Thank you, that's it for today.\" in last_msg.content:\n",
    "        return \"end\"\n",
    "    \n",
    "    # Continue conversation\n",
    "    return \"continue_convo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LangGraph workflow\n",
    "def build_workflow():\n",
    "    \"\"\"Build and compile the LangGraph workflow.\"\"\"\n",
    "    # Create the graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"recruiter\", recruiter_agent)\n",
    "    \n",
    "    # Add tool node\n",
    "    tool_node = ToolNode(tools)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"recruiter\")\n",
    "    \n",
    "    # Add conditional edges\n",
    "    workflow.add_conditional_edges(\n",
    "        \"recruiter\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"invoke_tools\": \"tools\",\n",
    "            \"continue_convo\": \"recruiter\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add edge from tools back to recruiter\n",
    "    workflow.add_edge(\"tools\", \"recruiter\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Build the workflow\n",
    "app = build_workflow()\n",
    "print(\"‚úÖ LangGraph workflow built successfully\")\n",
    "\n",
    "# Display the workflow diagram\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not display workflow diagram: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Speech Technologies <a name=\"speech\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-Speech (TTS) with ElevenLabs\n",
    "def setup_tts():\n",
    "    \"\"\"Setup ElevenLabs TTS client.\"\"\"\n",
    "    try:\n",
    "        from elevenlabs import ElevenLabs\n",
    "        \n",
    "        client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "        \n",
    "        # Test TTS\n",
    "        test_audio = client.text_to_speech.convert(\n",
    "            text=\"TTS system initialized successfully.\",\n",
    "            voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n",
    "            model_id=\"eleven_multilingual_v2\",\n",
    "            output_format=\"mp3_44100_128\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ ElevenLabs TTS initialized successfully\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up TTS: {e}\")\n",
    "        return None\n",
    "\n",
    "tts_client = setup_tts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech-to-Text (STT) with AssemblyAI\n",
    "def setup_stt():\n",
    "    \"\"\"Setup AssemblyAI STT client.\"\"\"\n",
    "    try:\n",
    "        import assemblyai as aai\n",
    "        \n",
    "        aai.settings.api_key = os.getenv(\"ASSEMBLYAI_API_KEY\")\n",
    "        transcriber = aai.Transcriber()\n",
    "        \n",
    "        print(\"‚úÖ AssemblyAI STT initialized successfully\")\n",
    "        return transcriber\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up STT: {e}\")\n",
    "        return None\n",
    "\n",
    "stt_transcriber = setup_stt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording and transcription function\n",
    "def record_and_transcribe(duration=5, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Record audio and transcribe using AssemblyAI.\n",
    "    \n",
    "    Args:\n",
    "        duration: Recording duration in seconds\n",
    "        sample_rate: Audio sample rate\n",
    "        \n",
    "    Returns:\n",
    "        Transcribed text or empty string if failed\n",
    "    \"\"\"\n",
    "    if not stt_transcriber:\n",
    "        print(\"‚ùå STT not available\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        import sounddevice as sd\n",
    "        import tempfile\n",
    "        from scipy.io.wavfile import write as write_wav\n",
    "        \n",
    "        print(f\"üéôÔ∏è Recording for {duration} seconds...\")\n",
    "        \n",
    "        # Record audio\n",
    "        audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='int16')\n",
    "        sd.wait()\n",
    "        print(\"‚úÖ Recording complete\")\n",
    "        \n",
    "        # Save to temporary file\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
    "            write_wav(tmp.name, sample_rate, audio)\n",
    "            audio_path = tmp.name\n",
    "        \n",
    "        # Transcribe\n",
    "        transcript = stt_transcriber.transcribe(audio_path)\n",
    "        \n",
    "        # Clean up\n",
    "        os.unlink(audio_path)\n",
    "        \n",
    "        if transcript.status == aai.TranscriptStatus.completed:\n",
    "            print(f\"üó£Ô∏è Transcription: {transcript.text}\")\n",
    "            return transcript.text\n",
    "        else:\n",
    "            print(f\"‚ùå Transcription failed: {transcript.error}\")\n",
    "            return \"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in recording/transcription: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Test STT\n",
    "print(\"Testing STT - please speak when prompted...\")\n",
    "test_transcript = record_and_transcribe()\n",
    "if test_transcript:\n",
    "    print(f\"‚úÖ STT test successful: '{test_transcript}'\")\n",
    "else:\n",
    "    print(\"‚ùå STT test failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration and Testing <a name=\"integration\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated chat loop with voice capabilities\n",
    "def voice_chat_loop(initial_state: AgentState, use_voice=True):\n",
    "    \"\"\"\n",
    "    Run a conversational loop with optional voice input/output.\n",
    "    \n",
    "    Args:\n",
    "        initial_state: Initial agent state\n",
    "        use_voice: Whether to use voice input/output\n",
    "        \n",
    "    Returns:\n",
    "        Final conversation state\n",
    "    \"\"\"\n",
    "    current_state = initial_state\n",
    "    \n",
    "    print(\"\\nüéØ Starting AI Interview System\")\n",
    "    print(f\"üìù Mode: {current_state['mode']}\")\n",
    "    print(f\"üè¢ Company: {current_state['company_name']}\")\n",
    "    print(f\"üíº Position: {current_state['position']}\")\n",
    "    print(f\"üé§ Voice enabled: {use_voice}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Get user input\n",
    "            if use_voice:\n",
    "                print(\"\\nüé§ Speak now (or type 'quit' to exit)...\")\n",
    "                user_input = record_and_transcribe()\n",
    "                if not user_input:\n",
    "                    print(\"‚ö†Ô∏è No speech detected, please try again\")\n",
    "                    continue\n",
    "            else:\n",
    "                user_input = input(\"\\nYou: \")\n",
    "            \n",
    "            # Check for exit command\n",
    "            if user_input.lower() in ['quit', 'exit', 'stop']:\n",
    "                print(\"\\nüëã Ending interview session\")\n",
    "                break\n",
    "            \n",
    "            print(f\"You: {user_input}\")\n",
    "            \n",
    "            # Add user message to state\n",
    "            current_state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "            \n",
    "            # Run the workflow\n",
    "            result = app.invoke(current_state)\n",
    "            current_state = result\n",
    "            \n",
    "            # Get AI response\n",
    "            ai_message = current_state[\"messages\"][-1]\n",
    "            \n",
    "            # Display AI response\n",
    "            print(\"\\nü§ñ AI Recruiter:\")\n",
    "            ai_message.pretty_print()\n",
    "            \n",
    "            # Generate and play audio if TTS is available\n",
    "            if use_voice and tts_client and isinstance(ai_message, AIMessage):\n",
    "                try:\n",
    "                    audio_stream = tts_client.text_to_speech.convert(\n",
    "                        text=ai_message.content,\n",
    "                        voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n",
    "                        model_id=\"eleven_multilingual_v2\",\n",
    "                        output_format=\"mp3_44100_128\"\n",
    "                    )\n",
    "                    \n",
    "                    # Collect audio bytes and play\n",
    "                    audio_bytes = b\"\".join(list(audio_stream))\n",
    "                    display(Audio(audio_bytes, autoplay=True))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è TTS error: {e}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ùå Interview interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error in chat loop: {e}\")\n",
    "    \n",
    "    return current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the system with a simple configuration\n",
    "def test_system():\n",
    "    \"\"\"Test the interview system with a basic configuration.\"\"\"\n",
    "    test_state = AgentState(\n",
    "        mode=\"friendly\",\n",
    "        num_of_q=2,\n",
    "        num_of_follow_up=1,\n",
    "        position=\"AI Developer\",\n",
    "        company_name=\"TechCorp\",\n",
    "        messages=[HumanMessage(content=\"Hello, I'm ready for the interview.\")]\n",
    "    )\n",
    "    \n",
    "    print(\"üß™ Testing interview system...\")\n",
    "    \n",
    "    # Test with text input first\n",
    "    final_state = voice_chat_loop(test_state, use_voice=False)\n",
    "    \n",
    "    print(\"\\nüìä Interview Summary:\")\n",
    "    print(f\"Total messages: {len(final_state['messages'])}\")\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "# Uncomment to run test\n",
    "# test_result = test_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usage Examples <a name=\"usage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Text-based interview\n",
    "def run_text_interview():\n",
    "    \"\"\"Run a text-based interview session.\"\"\"\n",
    "    initial_state = AgentState(\n",
    "        mode=\"professional\",\n",
    "        num_of_q=3,\n",
    "        num_of_follow_up=2,\n",
    "        position=\"Machine Learning Engineer\",\n",
    "        company_name=\"AI Innovations Inc.\",\n",
    "        messages=[]\n",
    "    )\n",
    "    \n",
    "    return voice_chat_loop(initial_state, use_voice=False)\n",
    "\n",
    "# Example 2: Voice-based interview\n",
    "def run_voice_interview():\n",
    "    \"\"\"Run a voice-based interview session.\"\"\"\n",
    "    initial_state = AgentState(\n",
    "        mode=\"friendly\",\n",
    "        num_of_q=2,\n",
    "        num_of_follow_up=1,\n",
    "        position=\"Data Scientist\",\n",
    "        company_name=\"DataTech Solutions\",\n",
    "        messages=[]\n",
    "    )\n",
    "    \n",
    "    return voice_chat_loop(initial_state, use_voice=True)\n",
    "\n",
    "# Example 3: Custom interview configuration\n",
    "def run_custom_interview(mode=\"friendly\", position=\"AI Developer\", company=\"OpenAI\", questions=5, followups=2):\n",
    "    \"\"\"Run a custom interview with specified parameters.\"\"\"\n",
    "    initial_state = AgentState(\n",
    "        mode=mode,\n",
    "        num_of_q=questions,\n",
    "        num_of_follow_up=followups,\n",
    "        position=position,\n",
    "        company_name=company,\n",
    "        messages=[]\n",
    "    )\n",
    "    \n",
    "    return voice_chat_loop(initial_state, use_voice=True)\n",
    "\n",
    "print(\"‚úÖ Usage examples defined\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"- run_text_interview(): Text-based interview\")\n",
    "print(\"- run_voice_interview(): Voice-based interview\")\n",
    "print(\"- run_custom_interview(): Custom configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "To start an interview session, run one of the following:\n",
    "\n",
    "```python\n",
    "# Text-based interview\n",
    "result = run_text_interview()\n",
    "\n",
    "# Voice-based interview\n",
    "result = run_voice_interview()\n",
    "\n",
    "# Custom interview\n",
    "result = run_custom_interview(\n",
    "    mode=\"technical\",\n",
    "    position=\"Senior AI Engineer\",\n",
    "    company=\"Google\",\n",
    "    questions=4,\n",
    "    followups=3\n",
    ")\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Make sure you have the following environment variables set in your `.env` file:\n",
    "\n",
    "- `GOOGLE_API_KEY`: For Gemini LLM\n",
    "- `ELEVENLABS_API_KEY`: For text-to-speech\n",
    "- `ASSEMBLYAI_API_KEY`: For speech-to-text\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "1. **STT not working**: Check your microphone permissions and AssemblyAI API key\n",
    "2. **TTS not working**: Verify your ElevenLabs API key and internet connection\n",
    "3. **LLM errors**: Ensure your Google API key is valid and has sufficient quota\n",
    "4. **File not found**: Make sure the PDF files are in the correct location (`../utils/`)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "This system can be extended with:\n",
    "- Real-time emotion detection\n",
    "- Interview scoring and feedback\n",
    "- Multi-language support\n",
    "- Integration with HR systems\n",
    "- Advanced analytics and reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 