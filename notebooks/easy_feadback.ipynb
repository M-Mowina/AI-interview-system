{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLP2N5CgYit5"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "epaRKJWSYit8"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from google.genai import types\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHP2rTSVYit9"
      },
      "source": [
        "# Intiate LLM client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xFr2yPzoYit-"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "client = genai.Client(api_key=google_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5492IeVLYit-"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "You are an AI interviewer for a leading tech company, conducting an interview for a Data Scientist position.\n",
        "\n",
        "Your goal is to assess the candidate's technical skills, problem-solving abilities, communication skills, and experience relevant to data science roles.\n",
        "\n",
        "Maintain a professional yet approachable tone. Start by introducing yourself as the interviewer and asking the candidate to introduce themselves and walk through their resume.\n",
        "\n",
        "Focus on questions related to:\n",
        "- Machine Learning concepts and algorithms\n",
        "- Statistical analysis and probability\n",
        "- Data manipulation and cleaning (Python, SQL)\n",
        "- Model evaluation and deployment\n",
        "- Problem-solving and case studies\n",
        "- Relevant projects and past experiences\n",
        "\n",
        "Begin the interview now.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "uPNwQVzPYit_",
        "outputId": "d9315947-692a-4405-b87a-1cdd56cf24a4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! Thank you for coming in today. My name is [Your Name/Interviewer's Name], and I'm a Data Scientist here at [Company Name]. I'll be conducting your interview for the Data Scientist position today.\n",
              "\n",
              "To start, could you please introduce yourself and walk me through your resume, highlighting the experiences you believe are most relevant to this role?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model='gemini-2.5-flash-lite-preview-06-17',\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=role),\n",
        "        contents=''\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lk6CnAQcH8f"
      },
      "source": [
        "## Chat with LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4czjE3itYit_"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(\n",
        "    model='gemini-2.5-flash-lite-preview-06-17',\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=role),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "49WDZY8PaKqx",
        "outputId": "e0aafa83-fa68-4a4d-cdb9-02dcf0ea2a79"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! Thanks for joining me today. I'm [Your Name], and I'll be your interviewer for the Data Scientist position. We're excited to learn more about your background and how your skills align with our team.\n",
              "\n",
              "To start, could you please introduce yourself and walk me through your resume, highlighting the experiences you believe are most relevant to this role?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"Hi\")\n",
        "display(Markdown (response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "JtDQ5WzraqPB",
        "outputId": "60136ae1-b52c-499a-957b-f79783479994"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Thanks, Mohamed. It's great to meet you. An automated interview assistant and an AI SIEM solution assistant sound like very interesting and impactful projects.\n",
              "\n",
              "Could you elaborate a bit more on your experience as an AI engineer and data scientist? Specifically, what types of models and techniques did you primarily work with in your freelance projects? And what did your role as an instructor involve?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response = chat.send_message(\"My name is mohamed mowina i am an AI engineer and a Data scientist I worked both as a freellancer and an instructor i built alot of Ai project like an automated interview assistant and an AI SIEM saloution assistant\")\n",
        "display(Markdown (response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "Vy-bT-_Gckc4",
        "outputId": "b4ded46b-1875-4bb8-8fae-af58c6726822"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "role - **user**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Hi"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "role - **model**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Hello! Thanks for joining me today. I'm [Your Name], and I'll be your interviewer for the Data Scientist position. We're excited to learn more about your background and how your skills align with our team.\n",
              "\n",
              "To start, could you please introduce yourself and walk me through your resume, highlighting the experiences you believe are most relevant to this role?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "role - **user**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "My name is mohamed mowina i am an AI engineer and a Data scientist I worked both as a freellancer and an instructor i built alot of Ai project like an automated interview assistant and an AI SIEM saloution assistant"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "role - **model**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Thanks, Mohamed. It's great to meet you. An automated interview assistant and an AI SIEM solution assistant sound like very interesting and impactful projects.\n",
              "\n",
              "Could you elaborate a bit more on your experience as an AI engineer and data scientist? Specifically, what types of models and techniques did you primarily work with in your freelance projects? And what did your role as an instructor involve?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for message in chat.get_history():\n",
        "    display(Markdown(f'role - **{message.role}**'))\n",
        "    display(Markdown(message.parts[0].text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store database in VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'chromadb'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpypdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerEmbeddingFunction\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chromadb'"
          ]
        }
      ],
      "source": [
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "import chromadb\n",
        "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Update the path to the correct location of your PDF file\n",
        "pdf_path = r\"C:\\Users\\mohamed mowina\\Desktop\\Projects\\AI-interview-system\\src\\LLM Interview Questions.pdf\"\n",
        "reader = PdfReader(pdf_path)\n",
        "pdf_texts = [p.extract_text().strip() for p in reader.pages]\n",
        "\n",
        "# Filter the empty strings\n",
        "pdf_texts = [text for text in pdf_texts if text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Top 50 Large Language Model (LLM) Interview\n",
              "Questions\n",
              "Hao Hoang -Follow me onLinkedIn for AI insights!\n",
              "May 2025\n",
              "Explore the key concepts, techniques, and challenges of Large Language Models (LLMs)\n",
              "with this comprehensive guide, crafted for AI enthusiasts and professionals preparing for\n",
              "interviews.\n",
              "Introduction\n",
              "Large Language Models (LLMs) are revolutionizing artiﬁcial intelligence, enabling ap-\n",
              "plications from chatbots to automated content creation. This document compiles 50\n",
              "essential interview questions, carefully curated to deepen your understanding of LLMs.\n",
              "Each question is paired with a detailed answer, blending technical insights with practical\n",
              "examples. Share this knowledge with your network to spark meaningful discussions in\n",
              "the AI community!\n",
              "1 Question 1: What does tokenization entail, and why is it\n",
              "critical for LLMs?\n",
              "Tokenization involves breaking down text into smaller units, or tokens, such as words,\n",
              "subwords, or characters. For example, \"artiﬁcial\" might be split into \"art,\" \"iﬁc,\" and\n",
              "\"ial.\" This process is vital because LLMs process numerical representations of tokens,\n",
              "not raw text. Tokenization enables models to handle diverse languages, manage rare or\n",
              "unknown words, and optimize vocabulary size, enhancing computational eﬃciency and\n",
              "model performance.\n",
              "2 Question 2: How does the attention mechanism function in\n",
              "transformer models?\n",
              "The attention mechanism allows LLMs to weigh the importance of diﬀerent tokens in a se-\n",
              "quence when generating or interpreting text. It computes similarity scores between query,\n",
              "key, and value vectors, using operations like dot products, to focus on relevant tokens.\n",
              "For instance, in \"The cat chased the mouse,\" attention helps the model link \"mouse\" to\n",
              "\"chased.\" This mechanism improves context understanding, making transformers highly\n",
              "eﬀective for NLP tasks.\n",
              "1"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(pdf_texts[0:5][0]))  # Display the first 5 pages of text to verify extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17 Question 17: How do transformers improve on traditional\n",
            "Seq2Seq models?\n",
            "Transformers overcome Seq2Seq limitations by:\n",
            "• Parallel Processing: Self-attention enables simultaneous token processing, unlike\n",
            "sequential RNNs.\n",
            "• Long-Range Dependencies: Attention captures distant token relationships.\n",
            "• Positional Encodings: These preserve sequence order.\n",
            "These features enhance scalability and performance in tasks like translation.\n",
            "18 Question 18: What is overﬁtting, and how can it be miti-\n",
            "gated in LLMs?\n",
            "Overﬁtting occurs when a model memorizes training data, failing to generalize. Mitigation\n",
            "includes:\n",
            "• Regularization: L1/L2 penalties simplify models.\n",
            "• Dropout: Randomly disables neurons during training.\n",
            "• Early Stopping: Halts training when validation performance plateaus.\n",
            "These techniques ensure robust generalization to unseen data.\n",
            "19 Question 19: What are generative versus discriminative mod-\n",
            "els in NLP?\n",
            "\n",
            "Total chunks: 25\n"
          ]
        }
      ],
      "source": [
        "character_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0\n",
        ")\n",
        "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
        "\n",
        "print(character_split_texts[10])\n",
        "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "17 question 17 : how do transformers improve on traditional seq2seq models? transformers overcome seq2seq limitations by : • parallel processing : self - attention enables simultaneous token processing, unlike sequential rnns. • long - range dependencies : attention captures distant token relationships. • positional encodings : these preserve sequence order. these features enhance scalability and performance in tasks like translation. 18 question 18 : what is overﬁtting, and how can it be miti - gated in llms? overﬁtting occurs when a model memorizes training data, failing to generalize. mitigation includes : • regularization : l1 / l2 penalties simplify models. • dropout : randomly disables neurons during training. • early stopping : halts training when validation performance plateaus. these techniques ensure robust generalization to unseen data. 19 question 19 : what are generative versus discriminative mod - els in nlp?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total chunks: 25\n"
          ]
        }
      ],
      "source": [
        "token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0, tokens_per_chunk=256)\n",
        "\n",
        "token_split_texts = []\n",
        "for text in character_split_texts:\n",
        "    token_split_texts += token_splitter.split_text(text)\n",
        "\n",
        "display(Markdown(token_split_texts[10]))\n",
        "print(f\"\\nTotal chunks: {len(token_split_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using voice for chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9I7xXDfbfZYu"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr # Import the speech_recognition library\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing microphone. Please say something...\n",
            "Microphone is working! You said: hi my name is Muhammad can you please\n"
          ]
        }
      ],
      "source": [
        "# test microphone\n",
        "def test_microphone():\n",
        "    r = sr.Recognizer()\n",
        "    try:\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Testing microphone. Please say something...\")\n",
        "            r.adjust_for_ambient_noise(source, duration=1)\n",
        "            audio = r.listen(source, timeout=5)\n",
        "        try:\n",
        "            text = r.recognize_google(audio)\n",
        "            print(f\"Microphone is working! You said: {text}\")\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"Microphone is working, but could not understand the audio.\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Microphone is working, but could not request results; {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Microphone test failed: {e}\")\n",
        "\n",
        "test_microphone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SkCA8uF-c5_k"
      },
      "outputs": [],
      "source": [
        "def record_and_chat(chat):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Please speak now...\")\n",
        "        r.adjust_for_ambient_noise(source, duration=1)\n",
        "        audio = r.listen(source)\n",
        "    try:\n",
        "        # Use Google Web Speech API for recognition\n",
        "        text = r.recognize_google(audio)\n",
        "        print(f\"You said: {text}\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Sorry, could not understand the audio.\")\n",
        "        return\n",
        "    except sr.RequestError as e:\n",
        "        print(f\"Could not request results; {e}\")\n",
        "        return\n",
        "\n",
        "    # Send the recognized text to the Gemini chat\n",
        "    response = chat.send_message(text)\n",
        "    display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "7Wyt3KPjfhiv",
        "outputId": "7a7efc7d-d391-4272-f411-a2f4237fa757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please speak now...\n",
            "You said: I work on a lot of projects\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "That's great to hear you've been involved in many projects, Mohamed. To help me understand your experience better, could you pick one or two of those projects that you found particularly challenging or that you're most proud of?\n",
              "\n",
              "For each of those projects, could you tell me:\n",
              "\n",
              "1.  **What was the problem you were trying to solve?**\n",
              "2.  **What was your approach?** (e.g., what data did you use, what algorithms did you consider or implement, what was your methodology?)\n",
              "3.  **What were the key outcomes or results?**\n",
              "4.  **What did you learn from this project?**\n",
              "\n",
              "This will give me a much clearer picture of your practical skills and problem-solving abilities."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "record_and_chat(chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert model's responses to audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyttsx3\n",
        "\n",
        "def speak_text(text):\n",
        "    engine = pyttsx3.init()\n",
        "    engine.say(text)\n",
        "    engine.runAndWait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "speak_text(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "def speak_text_gtts(text):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(\"response.mp3\")\n",
        "    os.system(\"start response.mp3\")  # On Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "speak_text_gtts(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<generator object gTTS.stream at 0x0000027A50475770>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gTTS(text=response.text, lang='en').stream()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MyTorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
